<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title> Smart Fast-Forward Player </title>
  <style>
    body { background:#0e0e0e; color:#fff; font-family:sans-serif; text-align:center; margin-top:60px; }
    input[type="range"] { width:60%; margin-top:20px; }
    #keywordDisplay { margin-top:30px; font-size:1.6em; color:#ffd500; }
    #status { margin-top:20px; font-style:italic; }
  </style>
</head>
<body>
  <h1> Fast-Forward Audio Player</h1>

  <input type="file" id="audioFile" accept="audio/*"><br><br>


  <audio id="audio" controls src="mp3s/manwhothinks.mp3"></audio><br>

  <label>Playback Speed:</label>
  <input type="range" id="speed" min="0.5" max="4" step="0.1" value="1">
  <span id="label">1.0x</span><br><br>
  <label>Semantic Threshold:</label>
  <input type="range" id="thresholdSlider" min="50" max="90" step="1" value="75">
  <span id="thresholdLabel">75th percentile</span><br><br>
  
  <button id="pauseBtn" style="display:none; padding:10px 20px; font-size:16px; cursor:pointer;">Pause Smart Scrub</button>
  <div id="keywordDisplay"></div>

  <div id="status"></div>

  <script>
    const API_KEY = "fae6aef01046408cb7e2c932adcf6e42"; // replace this with your key https://www.assemblyai.com/
    const uploadUrl = "https://api.assemblyai.com/v2/upload";
    const transcribeUrl = "https://api.assemblyai.com/v2/transcript";

    const audio = document.getElementById('audio');
    const slider = document.getElementById('speed');
    const label = document.getElementById('label');
    const display = document.getElementById('keywordDisplay');
    const status = document.getElementById('status');
    const fileInput = document.getElementById('audioFile');
    const pauseBtn = document.getElementById('pauseBtn');
    const timeDisplay = document.getElementById('timeDisplay');
    const thresholdSlider = document.getElementById('thresholdSlider');
    const thresholdLabel = document.getElementById('thresholdLabel');
    const PRE_ROLL_SEC = 0.05; // Pre-roll before word for context
    

    // Web Audio API setup for smooth transitions
    /*const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const gainNode = audioContext.createGain();
    const source = audioContext.createMediaElementSource(audio);
    source.connect(gainNode);
    gainNode.connect(audioContext.destination);
    gainNode.gain.value = 1;*/

    let audioContext = null;
    let gainNode = null;
    let mediaSource = null;
    
    function initAudioGraph() {
      if (audioContext) return; // Only run once
    
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      gainNode = audioContext.createGain();
      mediaSource = audioContext.createMediaElementSource(audio);
    
      mediaSource.connect(gainNode);
      gainNode.connect(audioContext.destination);
      gainNode.gain.value = 1;
    }
    
    // Safari only allows AudioContext after user gesture
    document.addEventListener("click", () => {
      initAudioGraph();
      audioContext.resume();
    }, { once: true });


    // Smart-scrub playback via decoded buffer to avoid seek clicks
    let smartBuffer = null;                 // Decoded AudioBuffer of current track
    let smartSrc = null;                    // Current BufferSource for word playback
    let smartGain = null;                   // Gain node for word envelope
    let smartCleanupTimer = null;           // Cleanup timer after fade-out

    async function decodeAudioFromBlob(blob) {
      try {
        const arrayBuffer = await blob.arrayBuffer();
        return await audioContext.decodeAudioData(arrayBuffer);
      } catch (e) {
        console.error('Failed to decode audio blob', e);
        return null;
      }
    }

    async function decodeAudioFromUrl(url) {
      try {
        const resp = await fetch(url);
        const blob = await resp.blob();
        return await decodeAudioFromBlob(blob);
      } catch (e) {
        console.error('Failed to decode from URL', url, e);
        return null;
      }
    }

    function stopSmartSegment(fadeMs = 10) {
      if (!smartSrc || !smartGain) return;
      const now = audioContext.currentTime;
      const fade = Math.max(0.005, (fadeMs || 10) / 1000);
      try {
        smartGain.gain.cancelScheduledValues(now);
        // Ramp to 0 then stop shortly after
        smartGain.gain.setValueAtTime(smartGain.gain.value, now);
        smartGain.gain.linearRampToValueAtTime(0, now + fade);
        clearTimeout(smartCleanupTimer);
        smartCleanupTimer = setTimeout(() => {
          try { smartSrc.stop(); } catch {}
          try { smartSrc.disconnect(); } catch {}
          try { smartGain.disconnect(); } catch {}
          smartSrc = null;
          smartGain = null;
        }, fade * 1000 + 5);
      } catch {}
    }

    function playSmartSegment(startSec, durationMs) {
      if (!smartBuffer) return false;
      // Ensure any previous segment is stopped cleanly
      stopSmartSegment(8);
      const durationSec = Math.max(0.03, (durationMs || 200) / 1000);
      const offset = Math.max(0, Math.min(startSec || 0, smartBuffer.duration - 0.01));
      if (offset >= smartBuffer.duration) return false;

      const src = audioContext.createBufferSource();
      const segGain = audioContext.createGain();
      src.buffer = smartBuffer;
      src.connect(segGain);
      segGain.connect(audioContext.destination);

      // Envelope: small attack/release to avoid clicks
      const now = audioContext.currentTime;
      const fade = Math.min(0.03, durationSec / 6);
      segGain.gain.setValueAtTime(0, now);
      segGain.gain.linearRampToValueAtTime(1, now + fade);
      segGain.gain.setValueAtTime(1, now + Math.max(fade, durationSec - fade));
      segGain.gain.linearRampToValueAtTime(0, now + durationSec);

      try {
        src.start(now, offset, Math.min(durationSec, Math.max(0.01, smartBuffer.duration - offset)));
      } catch (e) {
        console.error('Failed to start smart segment', e);
        try { src.disconnect(); segGain.disconnect(); } catch {}
        return false;
      }

      // Track for stop/fade
      smartSrc = src;
      smartGain = segGain;
      // Auto cleanup
      src.onended = () => {
        try { src.disconnect(); segGain.disconnect(); } catch {}
        if (smartSrc === src) smartSrc = null;
        if (smartGain === segGain) smartGain = null;
      };
      return true;
    }

  let informativeWords = [];
    let allTranscriptWords = [];  // Cache of full transcript for recalculating with different speeds
    let timer = null;
    let isDwelling = false;
    let dwellTimeout = null;
    let fastSpeed = 1;
    let nextIndex = 0;
    let isSmartScrubPaused = false;
    let manualThresholdPercentile = null; // User override for threshold
    let hasUserAdjustedThreshold = false; // Track if user manually set threshold
    // Basic common words
    const commonWords = new Set(["the","and","is","in","on","it","of","to","a","an","i","you","that","this"]);

    // Time formatting helpers
    function formatTime(sec) {
      if (!isFinite(sec) || sec < 0) sec = 0;
      const m = Math.floor(sec / 60);
      const s = Math.floor(sec % 60);
      return `${m.toString().padStart(2,'0')}:${s.toString().padStart(2,'0')}`;
    }
    function updateTimeDisplayUI(currentSec, totalSec) {
      if (!timeDisplay) return;
      const cur = formatTime(currentSec || 0);
      const tot = formatTime(totalSec || audio.duration || 0);
      timeDisplay.textContent = `${cur} / ${tot}`;
    }

    // Pause button functionality
    pauseBtn.onclick = () => {
      if (isSmartScrubPaused) {
        // Resume
        isSmartScrubPaused = false;
        pauseBtn.textContent = "Pause Smart Scrub";
      } else {
        // Pause
        isSmartScrubPaused = true;
        pauseBtn.textContent = "Resume Smart Scrub";
        audio.pause();
        // Stop any currently playing smart segment to avoid tails/clicks
        stopSmartSegment(8);
        clearTimeout(dwellTimeout);
        isDwelling = false;
        // Reset gain smoothly
        gainNode.gain.cancelScheduledValues(audioContext.currentTime);
        gainNode.gain.setValueAtTime(gainNode.gain.value, audioContext.currentTime);
        gainNode.gain.linearRampToValueAtTime(1, audioContext.currentTime + 0.01);
      }
    };

    // Shared function to load and process audio file
    async function loadAndProcessAudio(blob) {
      try {
        // Decode for smart-scrub playback
        smartBuffer = await decodeAudioFromBlob(blob);
        // Initialize time display with total duration
        if (smartBuffer) updateTimeDisplayUI(0, smartBuffer.duration);
        
        status.textContent = "Uploading audio...";
        const audioUrl = await uploadToAssembly(blob);
        
        status.textContent = "Processing transcription...";
        const words = await getTranscript(audioUrl);
        
        // Display full transcript in console
        displayTranscriptInConsole(words);
        
        // Cache full transcript for recalculating with different speeds
        allTranscriptWords = words;
        
        // Build informative words using TF-IDF over time windows
        informativeWords = selectInformativeWordsTFIDF(words, fastSpeed);
        nextIndex = 0;
        status.textContent = "Done! You can now play and scrub through keywords.";
        console.log("Informative words:", informativeWords);
      } catch (error) {
        console.error("Error processing audio:", error);
        status.textContent = "Error processing audio. Please try again.";
      }
    }

    // Load default file on startup
    window.addEventListener('load', async () => {
      const defaultFile = 'mp3s/manwhothinks.mp3';
      try {
        status.textContent = "Loading default audio...";
        const response = await fetch(defaultFile);
        const blob = await response.blob();
        await loadAndProcessAudio(blob);
      } catch (error) {
        console.error("Error loading default file:", error);
        status.textContent = "Default file loaded. Upload a file to transcribe.";
      }
    });

    fileInput.onchange = async () => {
      const file = fileInput.files[0];
      if (!file) return;
      audio.src = URL.createObjectURL(file);
      await loadAndProcessAudio(file);
    };

    async function uploadToAssembly(file) {
      const resp = await fetch(uploadUrl, {
        method: "POST",
        headers: { "Authorization": API_KEY },
        body: file
      });
      const data = await resp.json();
      return data.upload_url;
    }

    async function getTranscript(audioUrl) {
      const resp = await fetch(transcribeUrl, {
        method: "POST",
        headers: { "Authorization": API_KEY, "Content-Type": "application/json" },
        body: JSON.stringify({
          audio_url: audioUrl,
          speaker_labels: false,
          punctuate: true,
          format_text: true,
          word_boost: [],
          boost_param: "default",
          disfluencies: false
        })
      });
      const job = await resp.json();

      
      let statusCheck;
      do {
        await new Promise(r => setTimeout(r, 5000));
        const poll = await fetch(`${transcribeUrl}/${job.id}`, {
          headers: { "Authorization": API_KEY }
        });
        statusCheck = await poll.json();
        if (statusCheck.status === "error" || statusCheck.status === "failed") {
          throw new Error(statusCheck.error || "Transcription failed");
        }
      } while (statusCheck.status !== "completed");

      return statusCheck.words.map(w => ({ text: w.text, start: w.start / 1000, end: w.end ? w.end / 1000 : undefined }));
    }

    // Display transcript in browser console
    function displayTranscriptInConsole(words) {
      if (!words || words.length === 0) {
        console.warn('No words in transcript');
        return;
      }
      
      words.forEach((word, index) => {
        // word.start and word.end are already in seconds (converted in getTranscript)
        const startTime = word.start.toFixed(2);
        const endTime = word.end ? word.end.toFixed(2) : '?';
        const confidence = word.confidence ? `${(word.confidence * 100).toFixed(0)}%` : 'N/A';
        
        console.log(`[${startTime}s - ${endTime}s] ${word.text} (${confidence})`);
        
        // Add spacing every 10 words
        if ((index + 1) % 10 === 0) console.log('');
      });
      
      console.log('%câ”'.repeat(50), 'color: #ffd500');
      console.groupEnd();
    }

    // --- TF-IDF scoring over time windows ---
    function selectInformativeWordsTFIDF(words, speed = 1, overridePercentile = null) {
      if (!Array.isArray(words) || words.length === 0) return [];

      // Normalize tokens and filter basic noise
      const alpha = /[a-z]/i;
      const tokens = words.map(w => ({
        text: (w.text || '').toLowerCase().replace(/[^a-z'\-]/gi, ''),
        start: w.start,
        end: w.end
      })).filter(w => w.text && alpha.test(w.text) && !commonWords.has(w.text));

      if (tokens.length === 0) return [];

      // Build time windows as pseudo-documents
      const totalDuration = Math.max(...tokens.map(t => t.start));
      const windowSizeSec = 10; // adjustable window size
      const numWindows = Math.max(1, Math.ceil((totalDuration + 0.0001) / windowSizeSec));

      /**
       * df: number of windows that contain the term at least once
       * tf: total count in entire transcript
       */
      const df = new Map();
      const tf = new Map();

      // Track which terms appear per window
      const windowSets = Array.from({ length: numWindows }, () => new Set());
      for (const t of tokens) {
        // Term Frequency
        tf.set(t.text, (tf.get(t.text) || 0) + 1);

        // Document Frequency across windows
        const idx = Math.min(numWindows - 1, Math.max(0, Math.floor(t.start / windowSizeSec)));
        windowSets[idx].add(t.text);
      }
      for (const s of windowSets) {
        for (const term of s) {
          df.set(term, (df.get(term) || 0) + 1);
        }
      }

      // Compute IDF with smoothing
      const N = numWindows;
      const idf = new Map();
      for (const [term, dfCount] of df.entries()) {
        idf.set(term, Math.log((N + 1) / (dfCount + 1)) + 1); // > 0
      }

      // Compute TF-IDF per term
      const tfidf = new Map();
      for (const [term, tfCount] of tf.entries()) {
        const score = tfCount * (idf.get(term) || 0);
        tfidf.set(term, score);
      }

      // Build distribution of scores to set a dynamic threshold based on speed
      const scores = Array.from(tfidf.values()).filter(v => isFinite(v) && v > 0);
      if (scores.length === 0) return tokens; // fallback: return all tokens (already stopword-filtered)
      scores.sort((a, b) => a - b);
      
      // Dynamic percentile based on playback speed (unless overridden)
      // At 2.1x: ~75th percentile (top 25% words)
      // At 3x: ~85th percentile (top 15% words)
      // At 4x: ~92th percentile (top 8% words)
      let p = Math.min(0.95, 0.50 + (speed - 2) * 0.15);
      if (overridePercentile !== null && overridePercentile >= 0.50 && overridePercentile <= 0.95) {
        p = overridePercentile; // Use user's manual override
      }
      const idx75 = Math.floor(p * (scores.length - 1));
      const threshold = scores[idx75];

      // Attach per-token score based on its term score; filter by threshold
      const informative = tokens
        .map(t => ({ ...t, score: tfidf.get(t.text) || 0 }))
        .filter(t => t.score >= threshold)
        .sort((a, b) => a.start - b.start);

      // Map back to expected shape { text, start } for existing logic
      return informative.map(({ text, start, end }) => ({ text, start, end }));
    }

   // TF-IDF --- END ---

    // Helper to recalculate informative words with current settings
    function recalcInformative(resetIndexToCurrent = true) {
      if (allTranscriptWords.length === 0) return;
      informativeWords = selectInformativeWordsTFIDF(
        allTranscriptWords,
        fastSpeed,
        hasUserAdjustedThreshold ? manualThresholdPercentile : null
      );
      if (resetIndexToCurrent) {
        nextIndex = 0;
        while (nextIndex < informativeWords.length && informativeWords[nextIndex].start <= audio.currentTime) {
          nextIndex++;
        }
      }
    }

    // Update threshold slider UI based on speed (if user hasn't manually adjusted)
    function updateThresholdUIFromSpeed() {
      const dynamicP = Math.min(0.95, 0.60 + (fastSpeed - 2) * 0.18);
      thresholdSlider.value = Math.round(dynamicP * 100);
      thresholdLabel.textContent = `${Math.round(dynamicP * 100)}th percentile`;
    }

    slider.oninput = () => {
      const sp = parseFloat(slider.value);
      label.textContent = sp.toFixed(1) + "x";
      fastSpeed = sp;
      updateThresholdUIFromSpeed(); // Update threshold slider to match speed
      if (sp > 2) {
        recalcInformative(true);
        if (!timer) startSmartScrub();
      } else {
        stopSmartScrub();
        audio.playbackRate = sp;
      }
    };

    // Semantic threshold slider handler
    thresholdSlider.oninput = () => {
      const val = parseInt(thresholdSlider.value, 10); // 50-95
      manualThresholdPercentile = val / 100;
      thresholdLabel.textContent = `${val}th percentile`;
      hasUserAdjustedThreshold = true;
      recalcInformative(false); // Recalc without resetting index
    };

    function startSmartScrub() {
      // In smart-scrub mode, we do NOT speed up audio between words.
      // We keep audio paused except when dwelling on an informative word at 1.0x.
      audio.pause();
      audio.playbackRate = 1.0;
      // Ensure audio context is running for buffer playback
      if (audioContext.state === 'suspended') {
        audioContext.resume().catch(()=>{});
      }
      pauseBtn.style.display = 'inline-block';
      isSmartScrubPaused = false;
      pauseBtn.textContent = "Pause Smart Scrub";

      // Use a shorter fixed interval and check speed dynamically inside
      timer = setInterval(() => {
        // Keep running even if audio is paused; scrubbing is driven by timer + seeks
        if (fastSpeed <= 2) return stopSmartScrub();
        if (isDwelling || isSmartScrubPaused) return; // wait until current dwell finishes or paused

        // Advance pointer to next informative word ahead of current time
        while (nextIndex < informativeWords.length && informativeWords[nextIndex].start <= audio.currentTime) {
          nextIndex++;
        }
        if (nextIndex >= informativeWords.length) return stopSmartScrub();

        const next = informativeWords[nextIndex];

        // Determine dwell length: keep words understandable with minimum duration
        let dwellMs;
        if (typeof next.end === 'number' && next.end > next.start) {
          const durMs = (next.end - next.start) * 1000;
          // Keep full word duration for clarity, but ensure reasonable bounds
          dwellMs = Math.max(200, Math.min(1400, durMs));
        } else {
          // Fallback to reasonable default
          dwellMs = 200;
        }

        // Prepare dwell using decoded buffer (no HTMLAudio seek) to avoid clicks
        display.textContent = next.text;
        audio.playbackRate = 1.0;
        
        // Ensure we have a decoded buffer; if not, try to decode current src lazily
        if (!smartBuffer && audio.src) {
          decodeAudioFromUrl(audio.src).then(buf => { smartBuffer = buf; });
        }
        
        // Update native control progress and UI time to the word's timestamp
        try { audio.currentTime = Math.max(0, next.start - PRE_ROLL_SEC); } catch {}
        updateTimeDisplayUI(next.start, smartBuffer ? smartBuffer.duration : audio.duration);

        // If buffer not ready yet, skip this cycle and ensure HTML audio stays paused
        if (!smartBuffer) { try { audio.pause(); } catch {} ; return; }

        // Play the word segment with built-in envelope
        const ok = playSmartSegment(next.start, dwellMs);
        if (!ok) {
          // Skip this cycle; keep HTML audio paused to avoid double playback
          try { audio.pause(); } catch {}
          return;
        }

        // Mark dwell only after successful start
        isDwelling = true;
        clearTimeout(dwellTimeout);
        dwellTimeout = setTimeout(() => {
          // End dwell: stop segment (with quick fade) and advance
          stopSmartSegment(8);
          isDwelling = false;
          nextIndex++;
        }, dwellMs);
      }, 50); // Fixed 50ms interval for responsive speed changes
    }

    function stopSmartScrub() {
      clearInterval(timer);
      timer = null;
      clearTimeout(dwellTimeout);
      isDwelling = false;
      dwellTimeout = null;
      display.textContent = "";
      pauseBtn.style.display = 'none';
      isSmartScrubPaused = false;
      // Stop any smart segment playback
      stopSmartSegment(8);
      // Reset gain to full volume smoothly
      gainNode.gain.cancelScheduledValues(audioContext.currentTime);
      gainNode.gain.setValueAtTime(gainNode.gain.value, audioContext.currentTime);
      gainNode.gain.linearRampToValueAtTime(1, audioContext.currentTime + 0.01);
      // Do not force playback state here beyond rate; leave play/pause to the user
      audio.playbackRate = Math.max(0.5, Math.min(4, parseFloat(slider.value) || 1));
    }
  </script>
</body>
</html>
